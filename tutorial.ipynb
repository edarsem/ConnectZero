{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERALL, EXPLAIN CODE SNIPPETS, have them interact with the rest of the algorithms, and explain the overall approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the environment, the task, reward, the MARL setting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Ingredient: Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Tree Search: from BFS to Alpha-Beta Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search\n",
    "\n",
    "Describe MCTS, link it to UCB, \n",
    "\n",
    "Adding heuristics to MCTS\n",
    "\n",
    "Mention decision time planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Ingredient: Function Approximation\n",
    "\n",
    "### Use of a value network and policy network in MCTS: the first step in AlphaGo\n",
    "\n",
    "Show that a poorly trained value network/policy network are improved by MCTS :\n",
    "- show that value estimates are improved by MCTS\n",
    "- show that policy estimates are improved by MCTS (compared to depth 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train these networks (for the first step in AlphaGo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Ingredient: Self-Play\n",
    "\n",
    "### How to generate training data\n",
    "\n",
    "### How to train the networks\n",
    "\n",
    "Mention the details and how they are related to the different algorithms we have seen during the course (is there a replay buffer, is it on policy or off policy, etc.)\n",
    "\n",
    "Mention background planning\n",
    "\n",
    "Mention (well, it's in it) policy gradients.--> show what happens when you do MCTS with the trained policy but at depth zero (i.e. without any search and only using the policy) and with a very shallow search...show the importance of search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result! AlphaZero\n",
    "\n",
    "### Describe AlphaZero\n",
    "decition time planning with MCTS, background planning with self-play, and the use of deep neural networks.\n",
    "\n",
    "Show a complete training\n",
    "\n",
    "### Variants (MuZero, Gumbel MuZero, other like Efficient Zero?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
