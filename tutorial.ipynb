{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERALL, EXPLAIN CODE SNIPPETS, have them interact with the rest of the algorithms, and explain the overall approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the environment, the task, reward, the MARL setting...   \n",
    "\n",
    "\n",
    "The problem illustrated in this notebook is a Multi-Agent Reinforcement Learning (MARL) consisting of 2 players playing one against the other at a game called Connect 4. Connect 4 is a two-player game. The goal is to get 4 tokens aligned horizontally, vertically or diagonaly. The token are placed turn by turn and fall vertically until the bottom or another token. If the grid is full and nobody won, the game is a draw. The grid is a rectangle 7 cells wide and 6 cells high. The observation space is a 6*7 matrix filled with 1 if the agent has token on this cell and 0 if either nobody or the other agent has a token on this cell. The action space is an interger between 0 and 6 included that indicates on each column to drop a token.  \n",
    "\n",
    "The reward is +1 for a win, 0 for a draw and -1 for a lose or for an illegal move. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Ingredient: Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Tree Search: from BFS to Alpha-Beta Pruning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a certain state of the grid, we can compute all the possible positions with each action we can take. Then we can do the same for each action of the oppenent. This way, we can build a tree with each node representing the state of the board and the edges an action taken. A naive solution would be to explore all the tree until a certain depth. This algorithm thus works by taking the This method called Breadth-First Search (BFS) is simple to implement but is both memory and computation expensive as the number of nodes is multiplied by 7 at each depth. If we compute a single combination of moves from the node until the end, we obtain the Depth-First Search (DFS). In this case, to compute the value at each node we proceed according to the following way : if it is a terminal node, we assign a value of & in case of a win, 0 for a draw and -1 for a less. For non terminal nodes, if it is the agent's turn we take the maximum value among its children. If it is the oppenent's turn, we take the minimum value. It requires less memory compared to BFS but it may waste a lot of time exploring unpromising branches. A way to solve this problem is a method called Alpha-Beta Pruning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search\n",
    "\n",
    "Describe MCTS, link it to UCB, \n",
    "\n",
    "Adding heuristics to MCTS\n",
    "\n",
    "Mention decision time planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Ingredient: Function Approximation\n",
    "\n",
    "### Use of a value network and policy network in MCTS: the first step in AlphaGo\n",
    "\n",
    "Show that a poorly trained value network/policy network are improved by MCTS :\n",
    "- show that value estimates are improved by MCTS\n",
    "- show that policy estimates are improved by MCTS (compared to depth 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train these networks (for the first step in AlphaGo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Ingredient: Self-Play\n",
    "\n",
    "### How to generate training data\n",
    "\n",
    "### How to train the networks\n",
    "\n",
    "Mention the details and how they are related to the different algorithms we have seen during the course (is there a replay buffer, is it on policy or off policy, etc.)\n",
    "\n",
    "Mention background planning\n",
    "\n",
    "Mention (well, it's in it) policy gradients.--> show what happens when you do MCTS with the trained policy but at depth zero (i.e. without any search and only using the policy) and with a very shallow search...show the importance of search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result! AlphaZero\n",
    "\n",
    "### Describe AlphaZero\n",
    "decition time planning with MCTS, background planning with self-play, and the use of deep neural networks.\n",
    "\n",
    "Show a complete training\n",
    "\n",
    "### Variants (MuZero, Gumbel MuZero, other like Efficient Zero?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
