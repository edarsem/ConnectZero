"""Self-Play Variants for AlphaZero Training."""

import os

import jax
import opax

from train_agent import train, collect_self_play_data, prepare_training_data, train_step
from utils import import_class, save_model, load_model, make_directories


def train_normal(game_class, agent_class, base_path, **train_kwargs):
    """Standard AlphaZero self-play training."""
    model_dir = make_directories(base_path, "normal")
    train(
        game_class=game_class,
        agent_class=agent_class,
        ckpt_filename=os.path.join(model_dir, "model_iteration_0.pkl"),
        **train_kwargs,
    )


def frozen_self_play(
    game_class,
    agent_class,
    base_path,
    freeze_iteration,
    total_iterations,
    **train_kwargs,
):
    """
    Frozen Self-Play:
    - Freeze the agent at `freeze_iteration`.
    - Train the current agent against the frozen agent.
    """
    model_dir = make_directories(base_path, "frozen_self_play")
    env = import_class(game_class)()
    agent = import_class(agent_class)(
        input_dims=env.observation().shape, num_actions=env.num_actions()
    )

    frozen_agent = None
    for iteration in range(total_iterations):
        print(f"Iteration {iteration}: Training and Generating Data")

        if iteration == freeze_iteration:
            print(f"Freezing agent at iteration {freeze_iteration}")
            save_model(agent, model_dir, freeze_iteration)
            frozen_agent = load_model(model_dir, freeze_iteration)
            frozen_agent = frozen_agent.eval()

        # Generate data with frozen agent
        opponent_agent = frozen_agent if frozen_agent else agent
        opponent_agent = opponent_agent.eval()

        rng_key = jax.random.PRNGKey(iteration)
        data = collect_self_play_data(
            opponent_agent,
            env,
            rng_key,
            train_kwargs["selfplay_batch_size"],
            train_kwargs["num_self_plays_per_iteration"],
            train_kwargs["num_simulations_per_move"],
        )

        # Prepare training data and update agent
        training_data = prepare_training_data(data, env)
        agent = train_one_iteration(agent, training_data)

        save_model(agent, model_dir, iteration)


def self_play_with_all(
    game_class, agent_class, base_path, total_iterations, **train_kwargs
):
    """
    Self-Play with All Previous Models:
    - Each iteration, data is generated by all previous agents.
    """
    model_dir = make_directories(base_path, "self_play_all")
    env = import_class(game_class)()
    agent = import_class(agent_class)(
        input_dims=env.observation().shape, num_actions=env.num_actions()
    )

    for iteration in range(total_iterations):
        print(f"Iteration {iteration}: Generating Data from All Previous Models")
        save_model(agent, model_dir, iteration)

        for prev_iter in range(iteration + 1):
            prev_agent = load_model(model_dir, prev_iter)
            rng_key = jax.random.PRNGKey(prev_iter)
            collect_self_play_data(
                prev_agent,
                env,
                rng_key,
                train_kwargs["selfplay_batch_size"] // (iteration + 1),
                train_kwargs["num_self_plays_per_iteration"] // (iteration + 1),
                train_kwargs["num_simulations_per_move"],
            )


def self_play_with_fixed_agents(
    game_class,
    agent_class,
    base_path,
    fixed_agents_dir,
    agent_type,
    **train_kwargs,
):
    """
    Self-Play with Fixed Weak/Strong Agents.
    """
    model_dir = make_directories(base_path, "fixed_agents")
    env = import_class(game_class)()
    agent = import_class(agent_class)(
        input_dims=env.observation().shape, num_actions=env.num_actions()
    )

    # Load fixed agent
    fixed_agent_path = os.path.join(fixed_agents_dir, f"{agent_type}.pkl")
    fixed_agent = load_model(fixed_agent_path, 0)

    print(f"Training agent against fixed {agent_type} agent")

    for iteration in range(train_kwargs["num_iterations"]):
        print(f"Iteration {iteration}: Self-play against {agent_type}")
        rng_key = jax.random.PRNGKey(iteration)
        data = collect_self_play_data(
            fixed_agent,
            env,
            rng_key,
            train_kwargs["selfplay_batch_size"],
            train_kwargs["num_self_plays_per_iteration"],
            train_kwargs["num_simulations_per_move"],
        )

        # Prepare training data and update agent
        training_data = prepare_training_data(data, env)
        agent = train_one_iteration(agent, training_data)

        save_model(agent, model_dir, iteration)


def train_one_iteration(agent, training_data):
    """Perform one training iteration."""

    learning_rate = 0.01
    optim = opax.chain(
        opax.add_decayed_weights(1e-4),
        opax.sgd(learning_rate, momentum=0.9),
    ).init(agent.parameters())

    for batch in training_data:
        agent, optim, _ = train_step(agent, optim, batch)
    return agent


if __name__ == "__main__":
    BASE_PATH = "./models"
    GAME_CLASS = "games.connect_four_game.Connect4Game"
    AGENT_CLASS = "policies.resnet_policy.ResnetPolicyValueNet"

    TRAIN_KWARGS = {
        "selfplay_batch_size": 128,
        "num_self_plays_per_iteration": 1280,
        "num_simulations_per_move": 32,
        "num_iterations": 10,
        "num_eval_games": 128,
    }

    print("Starting Normal Self-Play...")
    train_normal(GAME_CLASS, AGENT_CLASS, BASE_PATH, **TRAIN_KWARGS)

    print("Starting Frozen Self-Play...")
    frozen_self_play(
        GAME_CLASS, AGENT_CLASS, BASE_PATH, freeze_iteration=5, total_iterations=10, **TRAIN_KWARGS
    )

    print("Starting Self-Play with All Previous Models...")
    self_play_with_all(GAME_CLASS, AGENT_CLASS, BASE_PATH, total_iterations=10, **TRAIN_KWARGS)

    # print("Starting Self-Play with Fixed Agents...")
    # FIXED_AGENTS_DIR = "./models/fixed_agents"
    # self_play_with_fixed_agents(
    #     GAME_CLASS, AGENT_CLASS, BASE_PATH, FIXED_AGENTS_DIR, agent_type="weak_agent", **TRAIN_KWARGS
    # )